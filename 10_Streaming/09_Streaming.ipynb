{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 09. Spark Streaming\n",
    "\n",
    "9.1 Extend the Spark Dataframe Example for counting HTTP response code in the NASA log files to Spark Structured Streaming. Use the [DataStreamReader] (https://spark.apache.org/docs/latest/api/python/pyspark.sql.html#pyspark.sql.streaming.DataStreamReader) API and the output mode \"update\"!\n",
    "\n",
    "* https://spark.apache.org/docs/latest/structured-streaming-programming-guide.html\n",
    "* https://spark.apache.org/docs/latest/api/python/pyspark.sql.html#module-pyspark.sql.streaming\n",
    "* https://spark.apache.org/docs/latest/api/python/pyspark.sql.html#pyspark.sql.streaming.DataStreamReader    \n",
    "* `spark_streaming.py`\n",
    "    \n",
    "    \n",
    "9.1 Add an additional copy of the NASA file to your Streaming data source directory! What do you observer?\n",
    "\n",
    "9.2 Create a subset of the data (10 rows) and slightly modify it! Add it to the source directory!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import time\n",
    "import datetime\n",
    "import logging\n",
    "import numpy as np\n",
    "import socket\n",
    "import re\n",
    "from subprocess import check_output\n",
    "\n",
    "from pyspark.mllib.linalg import Vectors\n",
    "from pyspark.mllib.regression import LabeledPoint\n",
    "from pyspark.mllib.clustering import StreamingKMeans\n",
    "from pyspark.streaming import StreamingContext\n",
    "from pyspark.streaming.kafka import KafkaUtils\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import explode\n",
    "from pyspark.sql.functions import split\n",
    "from pyspark.sql.types import StructType\n",
    "from pyspark.ml.feature import FeatureHasher\n",
    "from pyspark.ml.clustering import KMeans\n",
    "from pyspark.ml.evaluation import ClusteringEvaluator\n",
    "from pyspark.ml import Pipeline\n",
    "from pyspark.sql import functions as F\n",
    "from pyspark.sql.streaming import DataStreamReader\n",
    "\n",
    "#######################################################################################\n",
    "# CONFIGURATIONS\n",
    "# Get current cluster setup from work directory\n",
    "STREAMING_WINDOW=60\n",
    "\n",
    "# Initialize PySpark\n",
    "SPARK_MASTER=\"local[1]\"\n",
    "#SPARK_MASTER=\"spark://mpp3r03c04s06.cos.lrz.de:7077\"\n",
    "APP_NAME = \"PySpark Lecture\"\n",
    "os.environ[\"PYSPARK_PYTHON\"] = \"/opt/tljh/user/bin/python\"\n",
    "\n",
    "# If there is no SparkSession, create the environment\n",
    "try:\n",
    "    sc and spark\n",
    "except NameError as e:\n",
    "    import pyspark\n",
    "    import pyspark.sql\n",
    "    conf=pyspark.SparkConf().set(\"spark.cores.max\", \"4\")\n",
    "    sc = pyspark.SparkContext(master=SPARK_MASTER, conf=conf)\n",
    "    spark = pyspark.sql.SparkSession(sc).builder.appName(APP_NAME).getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "199.72.81.55 - - [01/Jul/1995:00:00:01 -0400] \"GET /history/apollo/ HTTP/1.0\" 200 6245\n",
      "unicomp6.unicomp.net - - [01/Jul/1995:00:00:06 -0400] \"GET /shuttle/countdown/ HTTP/1.0\" 200 3985\n",
      "199.120.110.21 - - [01/Jul/1995:00:00:09 -0400] \"GET /shuttle/missions/sts-73/mission-sts-73.html HTTP/1.0\" 200 4085\n",
      "burger.letters.com - - [01/Jul/1995:00:00:11 -0400] \"GET /shuttle/countdown/liftoff.html HTTP/1.0\" 304 0\n",
      "199.120.110.21 - - [01/Jul/1995:00:00:11 -0400] \"GET /shuttle/missions/sts-73/sts-73-patch-small.gif HTTP/1.0\" 200 4179\n",
      "burger.letters.com - - [01/Jul/1995:00:00:12 -0400] \"GET /images/NASA-logosmall.gif HTTP/1.0\" 304 0\n",
      "burger.letters.com - - [01/Jul/1995:00:00:12 -0400] \"GET /shuttle/countdown/video/livevideo.gif HTTP/1.0\" 200 0\n",
      "205.212.115.106 - - [01/Jul/1995:00:00:12 -0400] \"GET /shuttle/countdown/countdown.html HTTP/1.0\" 200 3985\n",
      "d104.aa.net - - [01/Jul/1995:00:00:13 -0400] \"GET /shuttle/countdown/ HTTP/1.0\" 200 3985\n",
      "129.94.144.152 - - [01/Jul/1995:00:00:13 -0400] \"GET / HTTP/1.0\" 200 7074\n"
     ]
    }
   ],
   "source": [
    "!head /opt/data/nasa/NASA_access_log_Jul95"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "lines = spark \\\n",
    "        .readStream \\\n",
    "        .format(\"text\") \\\n",
    "        .load(path=\"./streaming_src/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "words = lines.select(\n",
    "    explode(\n",
    "        split(lines.value, ' ')\n",
    "    ).alias('word')\n",
    ")\n",
    "\n",
    "wordCounts = words.groupBy('word').count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Start running the query that prints the running counts to the console\n",
    "query = wordCounts \\\n",
    "    .writeStream \\\n",
    "    .outputMode('complete') \\\n",
    "    .format('console')\\\n",
    "    .start()\n",
    "\n",
    "query.awaitTermination()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
