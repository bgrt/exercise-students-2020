{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5.SQL and Dataframes\n",
    "\n",
    "References:\n",
    "\n",
    "* Spark-SQL, <https://spark.apache.org/docs/latest/sql-programming-guide.html#datasets-and-dataframes>\n",
    "\n",
    "\n",
    "# 5.1  Example Walkthrough\n",
    "Follow the Spark SQL and Dataframes Examples below!\n",
    "\n",
    "### Initialize PySpark\n",
    "\n",
    "First, we use the findspark package to initialize PySpark."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/home/jupyter-ter-akopyan-118531-8d8bf/exercise-students-2020/05_SQL'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os, sys\n",
    "os.chdir(os.path.join(os.environ[\"HOME\"], \"exercise-students-2020/05_SQL\"))\n",
    "%pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PySpark initiated...\n"
     ]
    }
   ],
   "source": [
    "# Initialize PySpark\n",
    "import os, sys\n",
    "APP_NAME = \"PySpark Lecture\"\n",
    "SPARK_MASTER=\"local[1]\"\n",
    "import pyspark\n",
    "import pyspark.sql\n",
    "from pyspark.sql import Row\n",
    "conf=pyspark.SparkConf()\n",
    "conf=pyspark.SparkConf().setAppName(APP_NAME).set(\"spark.local.dir\", os.path.join(os.getcwd(), \"tmp\"))\n",
    "sc = pyspark.SparkContext(master=SPARK_MASTER, conf=conf)\n",
    "spark = pyspark.sql.SparkSession(sc).builder.appName(APP_NAME).getOrCreate()\n",
    "\n",
    "print(\"PySpark initiated...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hello, World!\n",
    "\n",
    "Loading data, mapping it and collecting the records into RAM..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['Russell Jurney', 'Relato', 'CEO'],\n",
       " ['Florian Liebert', 'Mesosphere', 'CEO'],\n",
       " ['Don Brown', 'Rocana', 'CIO'],\n",
       " ['Steve Jobs', 'Apple', 'CEO'],\n",
       " ['Donald Trump', 'The Trump Organization', 'CEO'],\n",
       " ['Russell Jurney', 'Data Syndrome', 'Principal Consultant']]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load the text file using the SparkContext\n",
    "csv_lines = sc.textFile(\"../data/example.csv\")\n",
    "\n",
    "# Map the data to split the lines into a list\n",
    "data = csv_lines.map(lambda line: line.split(\",\"))\n",
    "\n",
    "# Collect the dataset into local RAM\n",
    "data.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating Rows\n",
    "\n",
    "Creating `pyspark.sql.Rows` out of your data so you can create DataFrames..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert the CSV into a pyspark.sql.Row\n",
    "def csv_to_row(line):\n",
    "    parts = line.split(\",\")\n",
    "    row = Row(\n",
    "      name=parts[0],\n",
    "      company=parts[1],\n",
    "      title=parts[2]\n",
    "    )\n",
    "    return row\n",
    "\n",
    "# Apply the function to get rows in an RDD\n",
    "rows = csv_lines.map(csv_to_row)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating DataFrames from RDDs\n",
    "\n",
    "Using the `RDD.toDF()` method to create a dataframe, registering the `DataFrame` as a temporary table with Spark SQL, and counting the jobs per person using Spark SQL."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(company='Relato', name='Russell Jurney', title='CEO'),\n",
       " Row(company='Mesosphere', name='Florian Liebert', title='CEO'),\n",
       " Row(company='Rocana', name='Don Brown', title='CIO'),\n",
       " Row(company='Apple', name='Steve Jobs', title='CEO'),\n",
       " Row(company='The Trump Organization', name='Donald Trump', title='CEO')]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Convert to a pyspark.sql.DataFrame\n",
    "rows_df = rows.toDF()\n",
    "rows_df.take(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------------+-----+\n",
      "|           name|total|\n",
      "+---------------+-----+\n",
      "|   Donald Trump|    1|\n",
      "|Florian Liebert|    1|\n",
      "|      Don Brown|    1|\n",
      "| Russell Jurney|    2|\n",
      "|     Steve Jobs|    1|\n",
      "+---------------+-----+\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[Row(name='Donald Trump', total=1),\n",
       " Row(name='Florian Liebert', total=1),\n",
       " Row(name='Don Brown', total=1),\n",
       " Row(name='Russell Jurney', total=2),\n",
       " Row(name='Steve Jobs', total=1)]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Register the DataFrame for Spark SQL\n",
    "rows_df.registerTempTable(\"executives\")\n",
    "\n",
    "# Generate a new DataFrame with SQL using the SparkSession\n",
    "job_counts = spark.sql(\"\"\"\n",
    "SELECT\n",
    "  name,\n",
    "  COUNT(*) AS total\n",
    "  FROM executives\n",
    "  GROUP BY name\n",
    "\"\"\")\n",
    "job_counts.show()\n",
    "\n",
    "# Go back to an RDD\n",
    "job_counts.rdd.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5.2-5.9 NASA DataSet\n",
    "\n",
    "5.2 Create a Spark-SQL table with fields for IP/Host and Response Code from the NASA Log file! "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "163.205.53.14 - - [28/Jul/1995:13:32:23 -0400] \"GET /images/KSC-logosmall.gif HTTP/1.0\" 200 1204\n",
      "tiger2.ocs.lsu.edu - - [28/Jul/1995:13:32:23 -0400] \"GET /shuttle/missions/missions.html HTTP/1.0\" 200 8677\n",
      "199.0.2.27 - - [28/Jul/1995:13:32:23 -0400] \"GET /images/ksclogo-medium.gif HTTP/1.0\" 200 5866\n",
      "tornado.umd.edu - - [28/Jul/1995:13:32:25 -0400] \"GET /shuttle/missions/sts-74/sts-74-patch-small.gif HTTP/1.0\" 200 5494\n",
      "alyssa.p"
     ]
    }
   ],
   "source": [
    "!tail -n5 /opt/data/nasa/NASA_access_log_Jul95"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the text file using the SparkContext\n",
    "nasa_lines = sc.textFile(\"/opt/data/nasa/NASA_access_log_Jul95\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert the CSV into a pyspark.sql.Row\n",
    "def nasa_to_row(line):\n",
    "    parts = line.split(\" \")\n",
    "    row = Row(\n",
    "      host=parts[0],\n",
    "      res=parts[-2]\n",
    "    )\n",
    "    return row\n",
    "\n",
    "# Apply the function to get rows in an RDD\n",
    "nasa_rows = nasa_lines.filter(lambda x: '- -' in x).map(nasa_to_row)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert to a pyspark.sql.DataFrame\n",
    "nasa_df = nasa_rows.toDF()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(host='199.72.81.55', res='200'),\n",
       " Row(host='unicomp6.unicomp.net', res='200'),\n",
       " Row(host='199.120.110.21', res='200'),\n",
       " Row(host='burger.letters.com', res='304'),\n",
       " Row(host='199.120.110.21', res='200')]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nasa_df.take(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "5.3 Run an SQL query that outputs the number of occurrences of each HTTP response code!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+-------+\n",
      "|res|  total|\n",
      "+---+-------+\n",
      "|200|1701534|\n",
      "|302|  46573|\n",
      "|501|     14|\n",
      "|404|  10845|\n",
      "|403|     54|\n",
      "|500|     62|\n",
      "|304| 132627|\n",
      "|400|      5|\n",
      "+---+-------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Register the DataFrame for Spark SQL\n",
    "nasa_df.registerTempTable(\"nasalogs\")\n",
    "\n",
    "# Generate a new DataFrame with SQL using the SparkSession\n",
    "res_counts = spark.sql(\"\"\"\n",
    "SELECT\n",
    "  res,\n",
    "  COUNT(*) AS total\n",
    "  FROM nasalogs\n",
    "  GROUP BY res\n",
    "\"\"\")\n",
    "res_counts.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "5.4 Cachen Sie den Dataframe und führen Sie dieselbe Query nochmals aus! Messen Sie die Laufzeit für das Cachen und für die Ausführungszeit der Query!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from time import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cache runtime: 11.741\n"
     ]
    }
   ],
   "source": [
    "cstart = time()\n",
    "nasa_df.cache().count()\n",
    "cend = time()\n",
    "print('Cache runtime: %.3f' % (cend - cstart))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+-------+\n",
      "|res|  total|\n",
      "+---+-------+\n",
      "|200|1701534|\n",
      "|302|  46573|\n",
      "|501|     14|\n",
      "|404|  10845|\n",
      "|403|     54|\n",
      "|500|     62|\n",
      "|304| 132627|\n",
      "|400|      5|\n",
      "+---+-------+\n",
      "\n",
      "Query runtime: 1.365\n"
     ]
    }
   ],
   "source": [
    "qstart = time()\n",
    "res_counts = spark.sql(\"\"\"\n",
    "SELECT\n",
    "  res,\n",
    "  COUNT(*) AS total\n",
    "  FROM nasalogs\n",
    "  GROUP BY res\n",
    "\"\"\")\n",
    "\n",
    "res_counts.show()\n",
    "qend = time()\n",
    "print('Query runtime: %.3f' % (qend - qstart))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "5.5. Implement the same Query using the Dataframe API!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(host='199.72.81.55', res='200'),\n",
       " Row(host='unicomp6.unicomp.net', res='200'),\n",
       " Row(host='199.120.110.21', res='200')]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nasa_df.take(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+-------+\n",
      "|res|  count|\n",
      "+---+-------+\n",
      "|200|1701534|\n",
      "|302|  46573|\n",
      "|501|     14|\n",
      "|404|  10845|\n",
      "|403|     54|\n",
      "|500|     62|\n",
      "|304| 132627|\n",
      "|400|      5|\n",
      "+---+-------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "nasa_df.groupBy('res').count().show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "5.6 Führen Sie diesselbe Query mit/ohne Cache und 8, 16 Cores aus! Dokumentieren und erklären Sie das Ergebnis!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_num_cores(core = 8, cache = False):\n",
    "    if cache:\n",
    "        nasa_df.cache().count()\n",
    "    else:\n",
    "        nasa_df.unpersist().count()\n",
    "    spark.sparkContext.master = \"loca[{0}]\".format(core)\n",
    "    start = time()\n",
    "    nasa_df.groupBy('res').count().show()\n",
    "    return time() - start"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+-------+\n",
      "|res|  count|\n",
      "+---+-------+\n",
      "|200|1701534|\n",
      "|302|  46573|\n",
      "|501|     14|\n",
      "|404|  10845|\n",
      "|403|     54|\n",
      "|500|     62|\n",
      "|304| 132627|\n",
      "|400|      5|\n",
      "+---+-------+\n",
      "\n",
      "Elapsed time: 12.063 s\n"
     ]
    }
   ],
   "source": [
    "print('Elapsed time: %.3f s' % evaluate_num_cores(8, cache = False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+-------+\n",
      "|res|  count|\n",
      "+---+-------+\n",
      "|200|1701534|\n",
      "|302|  46573|\n",
      "|501|     14|\n",
      "|404|  10845|\n",
      "|403|     54|\n",
      "|500|     62|\n",
      "|304| 132627|\n",
      "|400|      5|\n",
      "+---+-------+\n",
      "\n",
      "Elapsed time: 0.881 s\n"
     ]
    }
   ],
   "source": [
    "print('Elapsed time: %.3f s' % evaluate_num_cores(8, cache = True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+-------+\n",
      "|res|  count|\n",
      "+---+-------+\n",
      "|200|1701534|\n",
      "|302|  46573|\n",
      "|501|     14|\n",
      "|404|  10845|\n",
      "|403|     54|\n",
      "|500|     62|\n",
      "|304| 132627|\n",
      "|400|      5|\n",
      "+---+-------+\n",
      "\n",
      "Elapsed time: 11.817 s\n"
     ]
    }
   ],
   "source": [
    "print('Elapsed time: %.3f s' % evaluate_num_cores(16, cache = False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+-------+\n",
      "|res|  count|\n",
      "+---+-------+\n",
      "|200|1701534|\n",
      "|302|  46573|\n",
      "|501|     14|\n",
      "|404|  10845|\n",
      "|403|     54|\n",
      "|500|     62|\n",
      "|304| 132627|\n",
      "|400|      5|\n",
      "+---+-------+\n",
      "\n",
      "Elapsed time: 0.832 s\n"
     ]
    }
   ],
   "source": [
    "print('Elapsed time: %.3f s' % evaluate_num_cores(16, cache = True))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Explanation**<br>\n",
    "independently of number of cores used the execution time of counting the occurence of each resonse code <br>\n",
    "is roughly the same and is only depending on the fact whether the data is cached or not! <br>\n",
    "<br>\n",
    "@Michał: do u have any improvements?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "            <div>\n",
       "                <p><b>SparkSession - in-memory</b></p>\n",
       "                \n",
       "        <div>\n",
       "            <p><b>SparkContext</b></p>\n",
       "\n",
       "            <p><a href=\"http://host-192-168-128-212.openstacklocal:4040\">Spark UI</a></p>\n",
       "\n",
       "            <dl>\n",
       "              <dt>Version</dt>\n",
       "                <dd><code>v2.4.5</code></dd>\n",
       "              <dt>Master</dt>\n",
       "                <dd><code>loca[16]</code></dd>\n",
       "              <dt>AppName</dt>\n",
       "                <dd><code>PySpark Lecture</code></dd>\n",
       "            </dl>\n",
       "        </div>\n",
       "        \n",
       "            </div>\n",
       "        "
      ],
      "text/plain": [
       "<pyspark.sql.session.SparkSession at 0x7f00b2e7fac8>"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "5.7 Performance Analysis: \n",
    "* Create RDDs with 2x, 4x, 8x and 16x of the size of the NASA log dataset! Persist the dataset in the Spark Cache! Use an appropriate number of cores (e.g. 8 or 16)!\n",
    "* Measure and plot the response times for all datasets using a constant number of cores!\n",
    "* Plot the results!\n",
    "* Explain the results!\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "def oversample(n):\n",
    "    nasa_lines = sc.textFile(\"/opt/data/nasa/NASA_access_log_Jul95\").flatMap(lambda x: [x]*n)\n",
    "    nasa_rows = nasa_lines.filter(lambda x: '- -' in x).map(nasa_to_row)\n",
    "    return nasa_rows.toDF()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "oversamples = []\n",
    "for x in (2**p for p in range(1, 5)):\n",
    "    tmp = oversample(x)\n",
    "    tmp.cache().count()\n",
    "    oversamples.append(tmp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "measures = {}\n",
    "for i, df in enumerate(oversamples):\n",
    "    start = time()\n",
    "    df.groupBy('res').count()\n",
    "    end = time()\n",
    "    measures[2**(i + 1)] = end - start"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{2: 0.015379190444946289,\n",
       " 4: 0.012316703796386719,\n",
       " 8: 0.0066454410552978516,\n",
       " 16: 0.005765199661254883}"
      ]
     },
     "execution_count": 91,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "measures"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<BarContainer object of 4 artists>"
      ]
     },
     "execution_count": 97,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.bar(measures.keys(), measures.values())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "5.8 Strong Scaling\n",
    "\n",
    "  * **Measure the runtime for the query for 1, 2, 4, 8 and 16 cores for 1x and 16x datasets!** Datasets cached in Memory!\n",
    "  * Compute the speedup and efficiency!\n",
    "  * Plot the responses!\n",
    "  * Explain the results!"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
